"""LongMemEval assessment pipeline.

This module provides the assessment pipeline that orchestrates:
1. Session ingestion into memory
2. Question answering via the agent
3. Answer assessment via LLM-as-Judge
4. Result aggregation and metrics

The pipeline follows the Benchmark Wrapper Pattern from ARCHITECTURE.md Section 4.
"""

from __future__ import annotations

import logging
import time
from collections.abc import Callable
from dataclasses import dataclass, field
from datetime import datetime
from typing import TYPE_CHECKING, Any

from src.evaluation.judge import Judgment

if TYPE_CHECKING:
    from src.adapters.base import MemorySystemAdapter
    from src.benchmarks.longmemeval.dataset import (
        LongMemEvalDataset,
        LongMemEvalQuestion,
    )
    from src.benchmarks.longmemeval.wrapper import AgentAnswer, LLMClient, LongMemEvalAgent
    from src.evaluation.judge import LLMJudge

logger = logging.getLogger(__name__)


@dataclass(slots=True)
class QuestionResult:
    """Result for a single question assessment.

    Combines the agent's answer with the judge's assessment.

    Attributes:
        question_id: The question identifier
        question_text: The original question
        question_type: Category of question (e.g., single-session-user)
        ground_truth: Expected correct answer(s)
        agent_answer: The answer generated by the agent
        judgment: The LLM judge's assessment
        is_abstention_expected: Whether abstention was the correct behavior
        is_abstention_actual: Whether the agent abstained
        latency_ms: Time taken to answer in milliseconds
    """

    question_id: str
    question_text: str
    question_type: str
    ground_truth: list[str]
    agent_answer: str
    judgment: Judgment
    is_abstention_expected: bool
    is_abstention_actual: bool
    latency_ms: float
    metadata: dict[str, Any] = field(default_factory=dict)

    @property
    def is_correct(self) -> bool:
        """Whether the answer was judged correct."""
        from src.evaluation.judge import JudgmentResult

        return self.judgment.result == JudgmentResult.CORRECT

    @property
    def is_partial(self) -> bool:
        """Whether the answer was judged partially correct."""
        from src.evaluation.judge import JudgmentResult

        return self.judgment.result == JudgmentResult.PARTIAL

    @property
    def score(self) -> float:
        """The judgment score (0.0 to 1.0)."""
        return self.judgment.score

    @property
    def abstention_correct(self) -> bool:
        """Whether abstention behavior was appropriate."""
        return self.is_abstention_expected == self.is_abstention_actual


@dataclass(slots=True)
class AssessmentResult:
    """Complete assessment results for a benchmark run.

    Aggregates all question results with summary metrics.

    Attributes:
        dataset_subset: Which dataset subset was assessed (S or M)
        question_results: Results for each question
        total_questions: Total number of questions assessed
        ingestion_time_ms: Time to ingest all sessions
        assessment_time_ms: Total assessment time
        started_at: When the assessment started
        completed_at: When the assessment finished
        metadata: Additional assessment metadata
    """

    dataset_subset: str
    question_results: list[QuestionResult]
    total_questions: int
    ingestion_time_ms: float
    assessment_time_ms: float
    started_at: datetime
    completed_at: datetime
    metadata: dict[str, Any] = field(default_factory=dict)

    @property
    def correct_count(self) -> int:
        """Number of questions answered correctly."""
        return sum(1 for r in self.question_results if r.is_correct)

    @property
    def partial_count(self) -> int:
        """Number of questions answered partially correctly."""
        return sum(1 for r in self.question_results if r.is_partial)

    @property
    def incorrect_count(self) -> int:
        """Number of questions answered incorrectly."""
        return self.total_questions - self.correct_count - self.partial_count

    @property
    def accuracy(self) -> float:
        """Overall accuracy (correct / total)."""
        if self.total_questions == 0:
            return 0.0
        return self.correct_count / self.total_questions

    @property
    def mean_score(self) -> float:
        """Mean score across all questions."""
        if not self.question_results:
            return 0.0
        return sum(r.score for r in self.question_results) / len(self.question_results)

    @property
    def abstention_accuracy(self) -> float:
        """Accuracy of abstention decisions."""
        abstention_questions = [r for r in self.question_results if r.is_abstention_expected]
        if not abstention_questions:
            return 1.0  # No abstention questions = perfect by default
        return sum(1 for r in abstention_questions if r.abstention_correct) / len(
            abstention_questions
        )

    def scores_by_type(self) -> dict[str, float]:
        """Get mean score grouped by question type."""
        type_scores: dict[str, list[float]] = {}
        for r in self.question_results:
            if r.question_type not in type_scores:
                type_scores[r.question_type] = []
            type_scores[r.question_type].append(r.score)

        return {qtype: sum(scores) / len(scores) for qtype, scores in type_scores.items()}

    def get_summary(self) -> dict[str, Any]:
        """Get a summary of assessment results."""
        return {
            "dataset_subset": self.dataset_subset,
            "total_questions": self.total_questions,
            "correct": self.correct_count,
            "partial": self.partial_count,
            "incorrect": self.incorrect_count,
            "accuracy": self.accuracy,
            "mean_score": self.mean_score,
            "abstention_accuracy": self.abstention_accuracy,
            "scores_by_type": self.scores_by_type(),
            "ingestion_time_ms": self.ingestion_time_ms,
            "assessment_time_ms": self.assessment_time_ms,
            "started_at": self.started_at.isoformat(),
            "completed_at": self.completed_at.isoformat(),
        }


class BenchmarkPipeline:
    """Assessment pipeline for LongMemEval benchmark.

    Orchestrates the complete assessment process:
    1. Ingest conversation sessions into memory
    2. Answer benchmark questions using the memory system
    3. Assess answers using LLM-as-Judge
    4. Aggregate results and compute metrics

    Example:
        ```python
        from src.adapters.git_notes import GitNotesAdapter
        from src.benchmarks.longmemeval import load_longmemeval, BenchmarkPipeline
        from src.assessment import LLMJudge

        # Set up components
        adapter = GitNotesAdapter(repo_path="/path/to/repo")
        llm_client = OpenAIClient(model="gpt-5-nano")
        judge = LLMJudge(client=llm_client)

        # Load dataset and create pipeline
        dataset = load_longmemeval(subset="S")
        pipeline = BenchmarkPipeline(adapter, llm_client, judge)

        # Run assessment
        results = pipeline.run(dataset)
        print(f"Accuracy: {results.accuracy:.2%}")
        ```

    Attributes:
        adapter: Memory system adapter for storage/retrieval
        llm_client: LLM client for answer generation
        judge: LLM judge for answer assessment
    """

    def __init__(
        self,
        adapter: MemorySystemAdapter,
        llm_client: LLMClient,
        judge: LLMJudge,
        *,
        memory_search_limit: int = 10,
        min_relevance_score: float = 0.0,
        relevant_sessions_only: bool = True,
    ) -> None:
        """Initialize the assessment pipeline.

        Args:
            adapter: Memory system adapter
            llm_client: LLM client for answer generation
            judge: LLM judge for assessment
            memory_search_limit: Max memories to retrieve per question
            min_relevance_score: Minimum relevance threshold
            relevant_sessions_only: Filter to relevant session IDs
        """
        self._adapter = adapter
        self._llm_client = llm_client
        self._judge = judge
        self._memory_search_limit = memory_search_limit
        self._min_relevance_score = min_relevance_score
        self._relevant_sessions_only = relevant_sessions_only

    def run(
        self,
        dataset: LongMemEvalDataset,
        *,
        skip_cache: bool = False,
        progress_callback: Callable[[int, int], None] | None = None,
    ) -> AssessmentResult:
        """Run the complete assessment pipeline.

        Args:
            dataset: The LongMemEval dataset to assess
            skip_cache: If True, bypass judgment cache
            progress_callback: Optional callback(current, total) for progress

        Returns:
            AssessmentResult with all question results and metrics
        """
        from src.benchmarks.longmemeval.wrapper import LongMemEvalAgent

        started_at = datetime.now()
        logger.info(f"Starting LongMemEval assessment for subset {dataset.subset}")

        # Phase 1: Create agent and ingest sessions
        logger.info(f"Ingesting {len(dataset.sessions)} sessions...")
        ingestion_start = time.perf_counter()

        agent = LongMemEvalAgent(
            self._adapter,
            self._llm_client,
            memory_search_limit=self._memory_search_limit,
            min_relevance_score=self._min_relevance_score,
        )

        ingestion_results = agent.ingest_all_sessions(dataset.sessions)
        total_messages = sum(ingestion_results.values())

        ingestion_time_ms = (time.perf_counter() - ingestion_start) * 1000
        logger.info(
            f"Ingested {total_messages} messages from {len(dataset.sessions)} sessions "
            f"in {ingestion_time_ms:.1f}ms"
        )

        # Phase 2: Answer all questions
        logger.info(
            f"Answering {len(dataset.questions)} questions (this may take several minutes)..."
        )
        answers_with_timing: list[tuple[AgentAnswer, float]] = []

        for idx, question in enumerate(dataset.questions):
            answer_start = time.perf_counter()
            answer = agent.answer_question(
                question, relevant_sessions_only=self._relevant_sessions_only
            )
            answer_time_ms = (time.perf_counter() - answer_start) * 1000
            answers_with_timing.append((answer, answer_time_ms))

            if progress_callback:
                progress_callback(idx + 1, len(dataset.questions))

            # Log progress every 10 questions at INFO level for visibility
            if (idx + 1) % 10 == 0 or idx == 0:
                logger.info(
                    f"  Answered {idx + 1}/{len(dataset.questions)} questions "
                    f"({(idx + 1) / len(dataset.questions) * 100:.0f}%) - last: {answer_time_ms:.0f}ms"
                )

        logger.info(f"Answered {len(answers_with_timing)} questions")

        # Phase 3: Assess answers using LLM-as-Judge
        logger.info(
            f"Judging {len(dataset.questions)} answers with LLM-as-Judge "
            "(this may take several minutes)..."
        )
        judge_start = time.perf_counter()

        # Judge each answer with progress logging
        judgments: list[Judgment] = []
        for idx, ((answer, _), question) in enumerate(
            zip(answers_with_timing, dataset.questions, strict=True)
        ):
            # Use first ground truth as reference
            reference = question.ground_truth[0] if question.ground_truth else ""
            judgment = self._judge.judge(
                question.question_text, reference, answer.answer, skip_cache=skip_cache
            )
            judgments.append(judgment)

            # Log progress every 10 judgments
            if (idx + 1) % 10 == 0 or idx == 0:
                elapsed = (time.perf_counter() - judge_start) * 1000
                logger.info(
                    f"  Judged {idx + 1}/{len(dataset.questions)} answers "
                    f"({(idx + 1) / len(dataset.questions) * 100:.0f}%) - "
                    f"elapsed: {elapsed / 1000:.1f}s"
                )

        judge_time_ms = (time.perf_counter() - judge_start) * 1000
        logger.info(f"Judged {len(judgments)} answers in {judge_time_ms / 1000:.1f}s")

        # Phase 4: Build results
        question_results: list[QuestionResult] = []

        for (answer, latency), question, judgment in zip(
            answers_with_timing, dataset.questions, judgments, strict=True
        ):
            question_result = QuestionResult(
                question_id=question.question_id,
                question_text=question.question_text,
                question_type=question.question_type.value,
                ground_truth=question.ground_truth,
                agent_answer=answer.answer,
                judgment=judgment,
                is_abstention_expected=question.is_abstention,
                is_abstention_actual=answer.is_abstention,
                latency_ms=latency,
                metadata={
                    "retrieved_memories": answer.retrieved_memories,
                    "relevant_sessions": question.relevant_session_ids,
                    "model": answer.metadata.get("model", ""),
                },
            )
            question_results.append(question_result)

        completed_at = datetime.now()
        assessment_time_ms = (completed_at - started_at).total_seconds() * 1000

        assessment_result = AssessmentResult(
            dataset_subset=dataset.subset,
            question_results=question_results,
            total_questions=len(dataset.questions),
            ingestion_time_ms=ingestion_time_ms,
            assessment_time_ms=assessment_time_ms,
            started_at=started_at,
            completed_at=completed_at,
            metadata={
                "memory_search_limit": self._memory_search_limit,
                "min_relevance_score": self._min_relevance_score,
                "relevant_sessions_only": self._relevant_sessions_only,
                "total_sessions": len(dataset.sessions),
                "total_messages_ingested": total_messages,
                "cache_hits": sum(1 for j in judgments if j.cached),
                "cache_misses": sum(1 for j in judgments if not j.cached),
            },
        )

        logger.info(
            f"Assessment complete: {assessment_result.accuracy:.2%} accuracy, "
            f"{assessment_result.mean_score:.3f} mean score"
        )

        return assessment_result

    def run_single_question(
        self,
        agent: LongMemEvalAgent,
        question: LongMemEvalQuestion,
        *,
        skip_cache: bool = False,
    ) -> QuestionResult:
        """Assess a single question (useful for debugging).

        Args:
            agent: Pre-configured agent with ingested sessions
            question: The question to assess
            skip_cache: If True, bypass judgment cache

        Returns:
            QuestionResult for this question
        """
        # Answer the question
        answer_start = time.perf_counter()
        answer = agent.answer_question(
            question, relevant_sessions_only=self._relevant_sessions_only
        )
        latency_ms = (time.perf_counter() - answer_start) * 1000

        # Judge the answer
        reference = question.ground_truth[0] if question.ground_truth else ""
        judgment = self._judge.judge(
            question.question_text, reference, answer.answer, skip_cache=skip_cache
        )

        return QuestionResult(
            question_id=question.question_id,
            question_text=question.question_text,
            question_type=question.question_type.value,
            ground_truth=question.ground_truth,
            agent_answer=answer.answer,
            judgment=judgment,
            is_abstention_expected=question.is_abstention,
            is_abstention_actual=answer.is_abstention,
            latency_ms=latency_ms,
            metadata={
                "retrieved_memories": answer.retrieved_memories,
                "relevant_sessions": question.relevant_session_ids,
            },
        )
