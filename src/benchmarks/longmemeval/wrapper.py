"""LongMemEval agent wrapper for benchmark assessment.

This module provides the LongMemEvalAgent class that adapts a MemorySystemAdapter
to the LongMemEval benchmark interface. It handles conversation history ingestion,
memory search, and question answering with optional abstention detection.

The wrapper follows the benchmark pattern from ARCHITECTURE.md section 4.
"""

from __future__ import annotations

import logging
from dataclasses import dataclass, field
from typing import TYPE_CHECKING, Any, Protocol

if TYPE_CHECKING:
    from src.adapters.base import MemorySystemAdapter
    from src.benchmarks.longmemeval.dataset import (
        LongMemEvalQuestion,
        LongMemEvalSession,
    )

logger = logging.getLogger(__name__)


class LLMClient(Protocol):
    """Protocol for LLM client interface.

    Implementations should provide a complete() method that generates
    text responses from messages.
    """

    def complete(
        self,
        system: str,
        messages: list[dict[str, str]],
        temperature: float = 0.0,
    ) -> LLMResponse:
        """Generate a completion from the LLM.

        Args:
            system: System prompt providing context
            messages: List of message dicts with 'role' and 'content'
            temperature: Sampling temperature (0.0 for deterministic)

        Returns:
            LLMResponse with the generated content
        """
        ...


@dataclass(slots=True)
class LLMResponse:
    """Response from an LLM completion.

    Attributes:
        content: The generated text response
        model: The model that generated the response
        usage: Token usage statistics
    """

    content: str
    model: str = ""
    usage: dict[str, int] = field(default_factory=dict)


@dataclass(slots=True)
class AgentAnswer:
    """Answer generated by the agent for a benchmark question.

    Attributes:
        question_id: The question this answers
        answer: The generated answer text
        retrieved_memories: Number of memories used for context
        is_abstention: Whether the agent abstained from answering
        metadata: Additional answer metadata (timing, model, etc.)
    """

    question_id: str
    answer: str
    retrieved_memories: int
    is_abstention: bool = False
    metadata: dict[str, Any] = field(default_factory=dict)


class LongMemEvalAgent:
    """Wrapper adapting MemorySystemAdapter to LongMemEval interface.

    This agent:
    1. Ingests conversation sessions into memory
    2. Retrieves relevant memories for each question
    3. Generates answers using an LLM with memory context
    4. Detects abstention questions and handles appropriately

    Example:
        ```python
        adapter = GitNotesAdapter(repo_path="/path/to/repo")
        llm = OpenAIClient(model="gpt-4o")
        agent = LongMemEvalAgent(adapter, llm)

        # Ingest conversation history
        for session in dataset.sessions:
            agent.ingest_session(session)

        # Answer questions
        for question in dataset.questions:
            answer = agent.answer_question(question)
            print(f"{question.question_id}: {answer.answer}")
        ```

    Attributes:
        adapter: The memory system adapter for storage/retrieval
        llm: The LLM client for generating answers
        config: Agent configuration settings
    """

    # Default system prompt for question answering
    DEFAULT_SYSTEM_PROMPT = (
        "You are an AI assistant that answers questions based on past conversations. "
        "Use only the provided conversation context to answer. "
        "If the information is not available in the context, say 'I don't know' or "
        "'I cannot find this information in our conversations.'"
    )

    # Prompt for detecting if the question requires abstention
    ABSTENTION_DETECTION_PROMPT = (
        "Based on the conversation history, determine if this question can be answered. "
        "If the answer is not present in the conversations, respond with "
        "'I don't know' or 'This information was not discussed.'"
    )

    def __init__(
        self,
        adapter: MemorySystemAdapter,
        llm: LLMClient,
        *,
        memory_search_limit: int = 10,
        min_relevance_score: float = 0.0,
        system_prompt: str | None = None,
        include_session_context: bool = True,
    ) -> None:
        """Initialize the LongMemEval agent.

        Args:
            adapter: Memory system adapter for storage and retrieval
            llm: LLM client for generating answers
            memory_search_limit: Max memories to retrieve per question
            min_relevance_score: Minimum score threshold for retrieval
            system_prompt: Custom system prompt (uses default if None)
            include_session_context: Whether to filter by session ID
        """
        self._adapter = adapter
        self._llm = llm
        self._memory_search_limit = memory_search_limit
        self._min_relevance_score = min_relevance_score
        self._system_prompt = system_prompt or self.DEFAULT_SYSTEM_PROMPT
        self._include_session_context = include_session_context

        # Track ingested sessions for debugging
        self._ingested_sessions: set[str] = set()

    @property
    def adapter(self) -> MemorySystemAdapter:
        """Get the memory adapter."""
        return self._adapter

    @property
    def ingested_session_count(self) -> int:
        """Get the number of ingested sessions."""
        return len(self._ingested_sessions)

    def ingest_session(self, session: LongMemEvalSession) -> int:
        """Ingest a conversation session into memory.

        Each message in the session is stored as a separate memory entry
        with metadata including session ID, role, and timestamp.

        Args:
            session: The LongMemEval session to ingest

        Returns:
            Number of messages successfully ingested
        """
        ingested_count = 0

        for idx, message in enumerate(session.messages):
            # Format content with role prefix for better retrieval
            content = f"{message.role}: {message.content}"

            # Build metadata for filtering and context
            metadata: dict[str, Any] = {
                "session_id": session.session_id,
                "role": message.role,
                "message_index": idx,
            }

            # Add optional timestamp if available
            if message.timestamp:
                metadata["timestamp"] = message.timestamp

            # Add session timestamp if available
            if session.timestamp:
                metadata["session_timestamp"] = session.timestamp

            # Store in memory system
            result = self._adapter.add(content, metadata)
            if result.success:
                ingested_count += 1
            else:
                logger.warning(
                    f"Failed to ingest message {idx} from session {session.session_id}: "
                    f"{result.error}"
                )

        self._ingested_sessions.add(session.session_id)
        logger.debug(
            f"Ingested {ingested_count}/{len(session.messages)} messages "
            f"from session {session.session_id}"
        )

        return ingested_count

    def ingest_all_sessions(
        self, sessions: list[LongMemEvalSession], *, use_batch: bool = True
    ) -> dict[str, int]:
        """Ingest multiple sessions into memory.

        Args:
            sessions: List of sessions to ingest
            use_batch: If True and adapter supports it, use batch ingestion (much faster)

        Returns:
            Dictionary mapping session_id to ingested message count
        """
        # Check if adapter supports fast batch ingestion
        has_batch = hasattr(self._adapter, "add_batch_fast") and use_batch

        if has_batch:
            return self._ingest_all_sessions_batch(sessions)

        # Fallback to individual ingestion
        results: dict[str, int] = {}
        total_messages = 0

        for session in sessions:
            count = self.ingest_session(session)
            results[session.session_id] = count
            total_messages += count

        logger.info(f"Ingested {total_messages} messages from {len(sessions)} sessions")
        return results

    def _ingest_all_sessions_batch(
        self, sessions: list[LongMemEvalSession]
    ) -> dict[str, int]:
        """Ingest all sessions using optimized batch ingestion.

        This is ~30-100x faster than individual ingestion for adapters
        that support add_batch_fast() (e.g., GitNotesAdapter).

        Args:
            sessions: List of sessions to ingest

        Returns:
            Dictionary mapping session_id to ingested message count
        """
        # Collect all messages as (content, metadata) tuples
        items: list[tuple[str, dict[str, Any]]] = []
        session_counts: dict[str, int] = {s.session_id: 0 for s in sessions}

        for session in sessions:
            for idx, message in enumerate(session.messages):
                content = f"{message.role}: {message.content}"
                metadata: dict[str, Any] = {
                    "session_id": session.session_id,
                    "role": message.role,
                    "message_index": idx,
                    "namespace": "longmemeval",
                }
                if message.timestamp:
                    metadata["timestamp"] = message.timestamp
                if session.timestamp:
                    metadata["session_timestamp"] = session.timestamp

                items.append((content, metadata))
                session_counts[session.session_id] += 1

        logger.info(
            f"Batch ingesting {len(items)} messages from {len(sessions)} sessions..."
        )

        # Use batch ingestion
        results = self._adapter.add_batch_fast(  # type: ignore[attr-defined]
            items,
            batch_size=64,
            show_progress=True,
        )

        # Count successful ingestions per session
        success_counts: dict[str, int] = {s.session_id: 0 for s in sessions}
        item_idx = 0
        for session in sessions:
            for _ in session.messages:
                if results[item_idx].success:
                    success_counts[session.session_id] += 1
                item_idx += 1

        # Track ingested sessions
        for session in sessions:
            self._ingested_sessions.add(session.session_id)

        total_success = sum(success_counts.values())
        logger.info(
            f"Batch ingested {total_success}/{len(items)} messages "
            f"from {len(sessions)} sessions"
        )

        return success_counts

    def answer_question(
        self,
        question: LongMemEvalQuestion,
        *,
        relevant_sessions_only: bool = True,
    ) -> AgentAnswer:
        """Answer a benchmark question using retrieved memories.

        Retrieves relevant memories based on the question text, constructs
        a context window, and generates an answer using the LLM.

        Args:
            question: The LongMemEval question to answer
            relevant_sessions_only: If True, filter to relevant session IDs

        Returns:
            AgentAnswer with the generated response and metadata
        """
        # Build metadata filter if using session context
        metadata_filter: dict[str, Any] | None = None
        if relevant_sessions_only and question.relevant_session_ids:
            # Filter to only search in relevant sessions
            metadata_filter = {"session_id": question.relevant_session_ids}

        # Search for relevant memories
        memories = self._adapter.search(
            query=question.question_text,
            limit=self._memory_search_limit,
            min_score=self._min_relevance_score,
            metadata_filter=metadata_filter,
        )

        # Build context from retrieved memories
        if memories:
            context_parts = []
            for mem in memories:
                # Include session context if available
                session_id = mem.metadata.get("session_id", "unknown")
                context_parts.append(f"[Session {session_id}] {mem.content}")

            context = "\n\n".join(context_parts)
        else:
            context = "(No relevant conversation history found)"

        # Construct the prompt
        user_message = (
            f"Conversation History:\n{context}\n\n"
            f"Question: {question.question_text}\n\n"
            "Answer based only on the conversation history above. "
            "If the information is not present, say so clearly."
        )

        # Generate answer using LLM
        response = self._llm.complete(
            system=self._system_prompt,
            messages=[{"role": "user", "content": user_message}],
            temperature=0.0,  # Deterministic for reproducibility
        )

        # Detect if answer indicates abstention
        is_abstention = self._detect_abstention(response.content)

        return AgentAnswer(
            question_id=question.question_id,
            answer=response.content,
            retrieved_memories=len(memories),
            is_abstention=is_abstention,
            metadata={
                "model": response.model,
                "usage": response.usage,
                "expected_abstention": question.is_abstention,
                "relevant_sessions": question.relevant_session_ids,
                "question_type": question.question_type.value,
            },
        )

    def answer_all_questions(
        self,
        questions: list[LongMemEvalQuestion],
        *,
        relevant_sessions_only: bool = True,
    ) -> list[AgentAnswer]:
        """Answer all benchmark questions.

        Args:
            questions: List of questions to answer
            relevant_sessions_only: If True, filter to relevant session IDs

        Returns:
            List of AgentAnswer objects
        """
        answers = []
        for question in questions:
            answer = self.answer_question(
                question,
                relevant_sessions_only=relevant_sessions_only,
            )
            answers.append(answer)

        logger.info(f"Answered {len(answers)} questions")
        return answers

    def _detect_abstention(self, answer: str) -> bool:
        """Detect if an answer indicates abstention (no information available).

        Checks for common patterns indicating the model couldn't find
        the requested information.

        Args:
            answer: The generated answer text

        Returns:
            True if the answer indicates abstention
        """
        answer_lower = answer.lower()

        abstention_phrases = [
            "i don't know",
            "i do not know",
            "cannot find",
            "can't find",
            "not mentioned",
            "not discussed",
            "no information",
            "not available",
            "wasn't mentioned",
            "was not mentioned",
            "wasn't discussed",
            "was not discussed",
            "not in the conversation",
            "not present in",
            "unable to find",
            "cannot determine",
            "can't determine",
            "not specified",
            "not clear from",
            "no relevant",
        ]

        return any(phrase in answer_lower for phrase in abstention_phrases)

    def clear_memory(self) -> bool:
        """Clear all ingested memories (for test isolation).

        Returns:
            True if successful, False otherwise
        """
        result = self._adapter.clear()
        if result.success:
            self._ingested_sessions.clear()
            logger.debug("Cleared all memories")
        return result.success

    def get_stats(self) -> dict[str, Any]:
        """Get agent statistics.

        Returns:
            Dictionary with agent and memory statistics
        """
        adapter_stats = self._adapter.get_stats()
        return {
            "ingested_sessions": len(self._ingested_sessions),
            "memory_search_limit": self._memory_search_limit,
            "min_relevance_score": self._min_relevance_score,
            **adapter_stats,
        }
