"""LoCoMo assessment pipeline.

This module provides the assessment pipeline that orchestrates:
1. Conversation ingestion into memory (multi-session dialogues)
2. Question answering via the agent (5 QA categories)
3. Answer assessment via LLM-as-Judge
4. Result aggregation and metrics by category

The pipeline follows the Benchmark Wrapper Pattern from ARCHITECTURE.md Section 4.

Key differences from LongMemEval pipeline:
- Multi-session conversations (up to 35 sessions per conversation)
- 5 QA categories requiring category-specific metrics
- Adversarial question handling (Category 5)
- Evidence-based answer validation
"""

from __future__ import annotations

import logging
import time
from collections.abc import Callable
from dataclasses import dataclass, field
from datetime import datetime
from typing import TYPE_CHECKING, Any

from src.evaluation.judge import Judgment

if TYPE_CHECKING:
    from src.adapters.base import MemorySystemAdapter
    from src.benchmarks.locomo.dataset import (
        LoCoMoDataset,
        LoCoMoQuestion,
        QACategory,
    )
    from src.benchmarks.locomo.wrapper import (
        IngestionResult,
        LLMClient,
        LoCoMoAgent,
        LoCoMoAnswer,
    )
    from src.evaluation.judge import LLMJudge

logger = logging.getLogger(__name__)


@dataclass(slots=True)
class QuestionResult:
    """Result for a single LoCoMo question assessment.

    Combines the agent's answer with the judge's assessment.

    Attributes:
        question_id: The question identifier
        conversation_id: The source conversation ID
        question_text: The original question
        category: QA category (1-5)
        ground_truth: Expected correct answer
        agent_answer: The answer generated by the agent
        judgment: The LLM judge's assessment
        is_adversarial: Whether this was an adversarial question
        is_abstention_actual: Whether the agent abstained
        adversarial_handled: For adversarial Qs, whether premise was identified
        latency_ms: Time taken to answer in milliseconds
        evidence_sessions: Session numbers containing the answer
        metadata: Additional result metadata
    """

    question_id: str
    conversation_id: str
    question_text: str
    category: QACategory
    ground_truth: str
    agent_answer: str
    judgment: Judgment
    is_adversarial: bool
    is_abstention_actual: bool
    adversarial_handled: bool
    latency_ms: float
    evidence_sessions: list[int]
    metadata: dict[str, Any] = field(default_factory=dict)

    @property
    def is_correct(self) -> bool:
        """Whether the answer was judged correct."""
        from src.evaluation.judge import JudgmentResult

        return self.judgment.result == JudgmentResult.CORRECT

    @property
    def is_partial(self) -> bool:
        """Whether the answer was judged partially correct."""
        from src.evaluation.judge import JudgmentResult

        return self.judgment.result == JudgmentResult.PARTIAL

    @property
    def score(self) -> float:
        """The judgment score (0.0 to 1.0)."""
        return self.judgment.score

    @property
    def category_name(self) -> str:
        """Get the category name."""
        return self.category.name


@dataclass(slots=True)
class CategoryMetrics:
    """Metrics for a specific QA category.

    Attributes:
        category: The QA category
        total_questions: Number of questions in this category
        correct_count: Number of correct answers
        partial_count: Number of partial answers
        mean_score: Average score for this category
        mean_latency_ms: Average response time
    """

    category: QACategory
    total_questions: int
    correct_count: int
    partial_count: int
    mean_score: float
    mean_latency_ms: float

    @property
    def accuracy(self) -> float:
        """Accuracy for this category."""
        if self.total_questions == 0:
            return 0.0
        return self.correct_count / self.total_questions

    @property
    def category_name(self) -> str:
        """Get category name."""
        return self.category.name


@dataclass(slots=True)
class ConversationMetrics:
    """Metrics for a specific conversation.

    Attributes:
        conversation_id: The conversation ID
        sessions_ingested: Number of sessions ingested
        turns_ingested: Number of turns ingested
        questions_assessed: Number of questions from this conversation
        correct_count: Number of correct answers
        mean_score: Average score for this conversation
    """

    conversation_id: str
    sessions_ingested: int
    turns_ingested: int
    questions_assessed: int
    correct_count: int
    mean_score: float

    @property
    def accuracy(self) -> float:
        """Accuracy for this conversation."""
        if self.questions_assessed == 0:
            return 0.0
        return self.correct_count / self.questions_assessed


@dataclass(slots=True)
class AssessmentResult:
    """Complete assessment results for a LoCoMo benchmark run.

    Aggregates all question results with summary metrics by category
    and conversation.

    Attributes:
        question_results: Results for each question
        category_metrics: Metrics broken down by QA category
        conversation_metrics: Metrics broken down by conversation
        total_questions: Total number of questions assessed
        ingestion_time_ms: Time to ingest all conversations
        assessment_time_ms: Total assessment time
        started_at: When the assessment started
        completed_at: When the assessment finished
        metadata: Additional assessment metadata
    """

    question_results: list[QuestionResult]
    category_metrics: dict[str, CategoryMetrics]
    conversation_metrics: dict[str, ConversationMetrics]
    total_questions: int
    ingestion_time_ms: float
    assessment_time_ms: float
    started_at: datetime
    completed_at: datetime
    metadata: dict[str, Any] = field(default_factory=dict)

    @property
    def correct_count(self) -> int:
        """Number of questions answered correctly."""
        return sum(1 for r in self.question_results if r.is_correct)

    @property
    def partial_count(self) -> int:
        """Number of questions answered partially correctly."""
        return sum(1 for r in self.question_results if r.is_partial)

    @property
    def incorrect_count(self) -> int:
        """Number of questions answered incorrectly."""
        return self.total_questions - self.correct_count - self.partial_count

    @property
    def accuracy(self) -> float:
        """Overall accuracy (correct / total)."""
        if self.total_questions == 0:
            return 0.0
        return self.correct_count / self.total_questions

    @property
    def mean_score(self) -> float:
        """Mean score across all questions."""
        if not self.question_results:
            return 0.0
        return sum(r.score for r in self.question_results) / len(self.question_results)

    @property
    def adversarial_accuracy(self) -> float:
        """Accuracy on adversarial questions (correctly identifying false premises)."""
        adversarial = [r for r in self.question_results if r.is_adversarial]
        if not adversarial:
            return 1.0
        return sum(1 for r in adversarial if r.adversarial_handled) / len(adversarial)

    @property
    def mean_latency_ms(self) -> float:
        """Mean latency across all questions."""
        if not self.question_results:
            return 0.0
        return sum(r.latency_ms for r in self.question_results) / len(self.question_results)

    def scores_by_category(self) -> dict[str, float]:
        """Get mean score grouped by QA category."""
        return {cat_name: metrics.mean_score for cat_name, metrics in self.category_metrics.items()}

    def accuracy_by_category(self) -> dict[str, float]:
        """Get accuracy grouped by QA category."""
        return {cat_name: metrics.accuracy for cat_name, metrics in self.category_metrics.items()}

    def get_summary(self) -> dict[str, Any]:
        """Get a summary of assessment results."""
        return {
            "total_questions": self.total_questions,
            "correct": self.correct_count,
            "partial": self.partial_count,
            "incorrect": self.incorrect_count,
            "accuracy": self.accuracy,
            "mean_score": self.mean_score,
            "adversarial_accuracy": self.adversarial_accuracy,
            "mean_latency_ms": self.mean_latency_ms,
            "scores_by_category": self.scores_by_category(),
            "accuracy_by_category": self.accuracy_by_category(),
            "conversations_assessed": len(self.conversation_metrics),
            "ingestion_time_ms": self.ingestion_time_ms,
            "assessment_time_ms": self.assessment_time_ms,
            "started_at": self.started_at.isoformat(),
            "completed_at": self.completed_at.isoformat(),
        }


class LoCoMoPipeline:
    """Assessment pipeline for LoCoMo benchmark.

    Orchestrates the complete assessment process:
    1. Ingest multi-session conversations into memory
    2. Answer benchmark questions using the memory system
    3. Assess answers using LLM-as-Judge
    4. Aggregate results and compute metrics by category

    Example:
        ```python
        from src.adapters.git_notes import GitNotesAdapter
        from src.benchmarks.locomo import load_locomo, LoCoMoPipeline
        from src.evaluation.judge import LLMJudge

        # Set up components
        adapter = GitNotesAdapter(repo_path="/path/to/repo")
        llm_client = OpenAIClient(model="gpt-4o")
        judge = LLMJudge(client=llm_client)

        # Load dataset and create pipeline
        dataset = load_locomo()
        pipeline = LoCoMoPipeline(adapter, llm_client, judge)

        # Run assessment
        results = pipeline.run(dataset)
        print(f"Overall Accuracy: {results.accuracy:.2%}")
        print(f"Category 5 (Adversarial): {results.adversarial_accuracy:.2%}")
        ```

    Attributes:
        adapter: Memory system adapter for storage/retrieval
        llm_client: LLM client for answer generation
        judge: LLM judge for answer assessment
    """

    def __init__(
        self,
        adapter: MemorySystemAdapter,
        llm_client: LLMClient,
        judge: LLMJudge,
        *,
        memory_search_limit: int = 15,
        min_relevance_score: float = 0.0,
        use_category_prompts: bool = True,
        use_evidence_sessions: bool = False,
    ) -> None:
        """Initialize the LoCoMo assessment pipeline.

        Args:
            adapter: Memory system adapter
            llm_client: LLM client for answer generation
            judge: LLM judge for assessment
            memory_search_limit: Max memories to retrieve per question
            min_relevance_score: Minimum relevance threshold
            use_category_prompts: Whether to use category-specific prompts
            use_evidence_sessions: If True, filter to evidence sessions (oracle mode)
        """
        self._adapter = adapter
        self._llm_client = llm_client
        self._judge = judge
        self._memory_search_limit = memory_search_limit
        self._min_relevance_score = min_relevance_score
        self._use_category_prompts = use_category_prompts
        self._use_evidence_sessions = use_evidence_sessions

    def run(
        self,
        dataset: LoCoMoDataset,
        *,
        skip_cache: bool = False,
        max_conversations: int | None = None,
        categories: list[QACategory] | None = None,
        progress_callback: Callable[[int, int], None] | None = None,
    ) -> AssessmentResult:
        """Run the complete assessment pipeline.

        Args:
            dataset: The LoCoMo dataset to assess
            skip_cache: If True, bypass judgment cache
            max_conversations: Limit number of conversations (for testing)
            categories: Filter to specific QA categories (None = all)
            progress_callback: Optional callback(current, total) for progress

        Returns:
            AssessmentResult with all question results and metrics
        """
        from src.benchmarks.locomo.wrapper import LoCoMoAgent

        started_at = datetime.now()
        logger.info(f"Starting LoCoMo assessment with {dataset.conversation_count} conversations")

        # Limit conversations if requested (for testing)
        conversations = dataset.conversations
        if max_conversations:
            conversations = conversations[:max_conversations]

        # Phase 1: Create agent and ingest all conversations
        logger.info(f"Ingesting {len(conversations)} conversations...")
        ingestion_start = time.perf_counter()

        agent = LoCoMoAgent(
            self._adapter,
            self._llm_client,
            memory_search_limit=self._memory_search_limit,
            min_relevance_score=self._min_relevance_score,
            use_category_prompts=self._use_category_prompts,
        )

        ingestion_results = agent.ingest_all_conversations(conversations)
        total_turns = sum(r.turns_ingested for r in ingestion_results.values())
        total_sessions = sum(r.sessions_ingested for r in ingestion_results.values())

        ingestion_time_ms = (time.perf_counter() - ingestion_start) * 1000
        logger.info(
            f"Ingested {total_turns} turns from {total_sessions} sessions "
            f"across {len(conversations)} conversations in {ingestion_time_ms:.1f}ms"
        )

        # Phase 2: Collect questions (filter by category if specified)
        all_questions: list[LoCoMoQuestion] = []
        for conv in conversations:
            all_questions.extend(conv.questions)

        if categories:
            all_questions = [q for q in all_questions if q.category in categories]
            logger.info(
                f"Filtered to {len(all_questions)} questions in categories: "
                f"{[c.name for c in categories]}"
            )
        else:
            logger.info(f"Assessing all {len(all_questions)} questions")

        # Phase 3: Answer all questions
        logger.info(f"Answering {len(all_questions)} questions (this may take several minutes)...")
        answers_with_timing: list[tuple[LoCoMoAnswer, float]] = []

        for idx, question in enumerate(all_questions):
            answer_start = time.perf_counter()
            answer = agent.answer_question(
                question, use_evidence_sessions=self._use_evidence_sessions
            )
            answer_time_ms = (time.perf_counter() - answer_start) * 1000
            answers_with_timing.append((answer, answer_time_ms))

            if progress_callback:
                progress_callback(idx + 1, len(all_questions))

            # Log progress every 10 questions at INFO level for visibility
            if (idx + 1) % 10 == 0 or idx == 0:
                logger.info(
                    f"  Answered {idx + 1}/{len(all_questions)} questions "
                    f"({(idx + 1) / len(all_questions) * 100:.0f}%) - last: {answer_time_ms:.0f}ms"
                )

        logger.info(f"Answered {len(answers_with_timing)} questions")

        # Phase 4: Assess answers using LLM-as-Judge
        logger.info(
            f"Judging {len(all_questions)} answers with LLM-as-Judge "
            "(this may take several minutes)..."
        )
        judge_start = time.perf_counter()

        # Judge each answer with progress logging
        judgments: list[Judgment] = []
        for idx, ((answer, _), question) in enumerate(
            zip(answers_with_timing, all_questions, strict=True)
        ):
            # For adversarial questions, use the adversarial_answer as "correct"
            if question.is_adversarial and question.adversarial_answer:
                reference = question.adversarial_answer
            else:
                reference = question.answer

            judgment = self._judge.judge(
                question.question, reference, answer.answer, skip_cache=skip_cache
            )
            judgments.append(judgment)

            # Log progress every 10 judgments
            if (idx + 1) % 10 == 0 or idx == 0:
                elapsed = (time.perf_counter() - judge_start) * 1000
                logger.info(
                    f"  Judged {idx + 1}/{len(all_questions)} answers "
                    f"({(idx + 1) / len(all_questions) * 100:.0f}%) - "
                    f"elapsed: {elapsed / 1000:.1f}s"
                )

        judge_time_ms = (time.perf_counter() - judge_start) * 1000
        logger.info(f"Judged {len(judgments)} answers in {judge_time_ms / 1000:.1f}s")

        # Phase 5: Build question results
        question_results: list[QuestionResult] = []

        for (answer, latency), question, judgment in zip(
            answers_with_timing, all_questions, judgments, strict=True
        ):
            # For adversarial questions, check if the agent identified the false premise
            adversarial_handled = False
            if question.is_adversarial:
                # Look for signs that the agent identified the incorrect premise
                adversarial_handled = self._check_adversarial_handling(answer.answer)

            question_result = QuestionResult(
                question_id=question.question_id,
                conversation_id=question.conversation_id,
                question_text=question.question,
                category=question.category,
                ground_truth=question.answer,
                agent_answer=answer.answer,
                judgment=judgment,
                is_adversarial=question.is_adversarial,
                is_abstention_actual=answer.is_abstention,
                adversarial_handled=adversarial_handled,
                latency_ms=latency,
                evidence_sessions=list(question.evidence_session_nums),
                metadata={
                    "retrieved_memories": answer.retrieved_memories,
                    "evidence_dia_ids": question.evidence,
                    "model": answer.metadata.get("model", ""),
                },
            )
            question_results.append(question_result)

        # Phase 6: Compute category metrics
        category_metrics = self._compute_category_metrics(question_results)

        # Phase 7: Compute conversation metrics
        conversation_metrics = self._compute_conversation_metrics(
            question_results, ingestion_results
        )

        completed_at = datetime.now()
        assessment_time_ms = (completed_at - started_at).total_seconds() * 1000

        assessment_result = AssessmentResult(
            question_results=question_results,
            category_metrics=category_metrics,
            conversation_metrics=conversation_metrics,
            total_questions=len(all_questions),
            ingestion_time_ms=ingestion_time_ms,
            assessment_time_ms=assessment_time_ms,
            started_at=started_at,
            completed_at=completed_at,
            metadata={
                "memory_search_limit": self._memory_search_limit,
                "min_relevance_score": self._min_relevance_score,
                "use_category_prompts": self._use_category_prompts,
                "use_evidence_sessions": self._use_evidence_sessions,
                "total_conversations": len(conversations),
                "total_sessions": total_sessions,
                "total_turns_ingested": total_turns,
                "categories_assessed": ([c.name for c in categories] if categories else "all"),
                "cache_hits": sum(1 for j in judgments if j.cached),
                "cache_misses": sum(1 for j in judgments if not j.cached),
            },
        )

        logger.info(
            f"Assessment complete: {assessment_result.accuracy:.2%} accuracy, "
            f"{assessment_result.mean_score:.3f} mean score"
        )

        # Log per-category results
        for cat_name, metrics in category_metrics.items():
            logger.info(
                f"  {cat_name}: {metrics.accuracy:.2%} accuracy, {metrics.mean_score:.3f} score"
            )

        return assessment_result

    def run_single_question(
        self,
        agent: LoCoMoAgent,
        question: LoCoMoQuestion,
        *,
        skip_cache: bool = False,
    ) -> QuestionResult:
        """Assess a single question (useful for debugging).

        Args:
            agent: Pre-configured agent with ingested conversations
            question: The question to assess
            skip_cache: If True, bypass judgment cache

        Returns:
            QuestionResult for this question
        """
        # Answer the question
        answer_start = time.perf_counter()
        answer = agent.answer_question(question, use_evidence_sessions=self._use_evidence_sessions)
        latency_ms = (time.perf_counter() - answer_start) * 1000

        # Judge the answer
        if question.is_adversarial and question.adversarial_answer:
            reference = question.adversarial_answer
        else:
            reference = question.answer

        judgment = self._judge.judge(
            question.question, reference, answer.answer, skip_cache=skip_cache
        )

        adversarial_handled = False
        if question.is_adversarial:
            adversarial_handled = self._check_adversarial_handling(answer.answer)

        return QuestionResult(
            question_id=question.question_id,
            conversation_id=question.conversation_id,
            question_text=question.question,
            category=question.category,
            ground_truth=question.answer,
            agent_answer=answer.answer,
            judgment=judgment,
            is_adversarial=question.is_adversarial,
            is_abstention_actual=answer.is_abstention,
            adversarial_handled=adversarial_handled,
            latency_ms=latency_ms,
            evidence_sessions=list(question.evidence_session_nums),
            metadata={
                "retrieved_memories": answer.retrieved_memories,
                "evidence_dia_ids": question.evidence,
            },
        )

    def _check_adversarial_handling(self, answer: str) -> bool:
        """Check if an answer correctly identifies an adversarial question's false premise.

        Args:
            answer: The agent's answer

        Returns:
            True if the answer indicates recognition of the false premise
        """
        answer_lower = answer.lower()

        # Phrases indicating the agent caught the incorrect premise
        premise_detection_phrases = [
            "premise is incorrect",
            "incorrect premise",
            "that's not accurate",
            "that is not accurate",
            "doesn't match",
            "does not match",
            "contrary to",
            "that's not what",
            "that is not what",
            "inaccurate assumption",
            "false assumption",
            "not true that",
            "isn't true that",
            "never mentioned",
            "never said",
            "didn't say",
            "did not say",
            "that's not correct",
            "that is not correct",
            "incorrect assumption",
            "based on a false",
            "based on an incorrect",
        ]

        return any(phrase in answer_lower for phrase in premise_detection_phrases)

    def _compute_category_metrics(
        self, results: list[QuestionResult]
    ) -> dict[str, CategoryMetrics]:
        """Compute metrics for each QA category.

        Args:
            results: All question results

        Returns:
            Dictionary mapping category name to metrics
        """

        category_results: dict[QACategory, list[QuestionResult]] = {}
        for r in results:
            if r.category not in category_results:
                category_results[r.category] = []
            category_results[r.category].append(r)

        metrics: dict[str, CategoryMetrics] = {}
        for category, cat_results in category_results.items():
            correct = sum(1 for r in cat_results if r.is_correct)
            partial = sum(1 for r in cat_results if r.is_partial)
            mean_score = (
                sum(r.score for r in cat_results) / len(cat_results) if cat_results else 0.0
            )
            mean_latency = (
                sum(r.latency_ms for r in cat_results) / len(cat_results) if cat_results else 0.0
            )

            metrics[category.name] = CategoryMetrics(
                category=category,
                total_questions=len(cat_results),
                correct_count=correct,
                partial_count=partial,
                mean_score=mean_score,
                mean_latency_ms=mean_latency,
            )

        return metrics

    def _compute_conversation_metrics(
        self,
        results: list[QuestionResult],
        ingestion_results: dict[str, IngestionResult],
    ) -> dict[str, ConversationMetrics]:
        """Compute metrics for each conversation.

        Args:
            results: All question results
            ingestion_results: Ingestion results per conversation

        Returns:
            Dictionary mapping conversation ID to metrics
        """
        conv_results: dict[str, list[QuestionResult]] = {}
        for r in results:
            if r.conversation_id not in conv_results:
                conv_results[r.conversation_id] = []
            conv_results[r.conversation_id].append(r)

        metrics: dict[str, ConversationMetrics] = {}
        for conv_id, conv_results_list in conv_results.items():
            ingestion = ingestion_results.get(conv_id)
            correct = sum(1 for r in conv_results_list if r.is_correct)
            mean_score = (
                sum(r.score for r in conv_results_list) / len(conv_results_list)
                if conv_results_list
                else 0.0
            )

            metrics[conv_id] = ConversationMetrics(
                conversation_id=conv_id,
                sessions_ingested=ingestion.sessions_ingested if ingestion else 0,
                turns_ingested=ingestion.turns_ingested if ingestion else 0,
                questions_assessed=len(conv_results_list),
                correct_count=correct,
                mean_score=mean_score,
            )

        return metrics
