# Memory Benchmark Harness - Docker Compose Configuration
#
# Services:
#   - benchmark: Full GPU-enabled benchmark runner
#   - benchmark-lite: CPU-only for CI and quick runs
#   - dev: Development environment with live reload
#
# Usage:
#   Full benchmark with GPU:
#     docker compose up benchmark
#
#   CPU-only for CI:
#     docker compose up benchmark-lite
#
#   Development mode:
#     docker compose up dev
#
# Environment Variables:
#   - OPENAI_API_KEY: Required for LLM-as-Judge evaluation
#   - ANTHROPIC_API_KEY: Optional for Anthropic models
#   - HF_TOKEN: Optional for gated HuggingFace models

services:
  # Full GPU benchmark runner
  benchmark:
    build:
      context: ..
      dockerfile: docker/Dockerfile.gpu
    image: benchmark-harness:gpu
    container_name: benchmark-harness-gpu
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            count: all
            capabilities: [gpu]
    volumes:
    - ../data:/app/data:ro
    - ../results:/app/results:rw
    - ../config:/app/config:ro
    - benchmark-cache:/app/.cache/huggingface
    environment:
    - OPENAI_API_KEY=${OPENAI_API_KEY:-}
    - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
    - HF_TOKEN=${HF_TOKEN:-}
    - BENCHMARK_MODE=full
    networks:
    - benchmark-net
    restart: no

  # CPU-only benchmark for CI and quick runs
  benchmark-lite:
    build:
      context: ..
      dockerfile: docker/Dockerfile.cpu
    image: benchmark-harness:cpu
    container_name: benchmark-harness-cpu
    # Memory requirements for LongMemEval with git-notes adapter:
    # - Embedding model: ~400MB
    # - 200k embeddings (384-dim): ~300MB
    # - SQLite index: ~200MB
    # - Python overhead: ~500MB
    # Total: ~1.5GB minimum, 4GB recommended
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 4G
    volumes:
    - ../data:/app/data:rw
    - ../results:/app/results:rw
    - ../config:/app/config:ro
    # Context-Bench dataset (letta-evals repo)
    - ../letta-evals:/app/letta-evals:ro
    # Cache HuggingFace models to avoid re-downloading
    - benchmark-cache:/app/.cache/huggingface
    environment:
    - OPENAI_API_KEY=${OPENAI_API_KEY:-}
    - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
    - BENCHMARK_MODE=lite
    # HuggingFace cache location (inside container)
    - HF_HOME=/app/.cache/huggingface
    # Sentence-transformers cache (used by git-notes-memory embeddings)
    - SENTENCE_TRANSFORMERS_HOME=/app/.cache/huggingface
    networks:
    - benchmark-net
    restart: no

  # Development environment with live reload
  dev:
    build:
      context: ..
      dockerfile: docker/Dockerfile.cpu
      target: base
    image: benchmark-harness:dev
    container_name: benchmark-harness-dev
    volumes:
    # Mount source code for live reload
    - ..:/app:cached
    # Preserve venv across rebuilds
    - dev-venv:/app/.venv
    environment:
    - OPENAI_API_KEY=${OPENAI_API_KEY:-}
    - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
    - PYTHONDONTWRITEBYTECODE=1
    working_dir: /app
    command: >
      bash -c "
        uv venv /app/.venv 2>/dev/null || true;
        . /app/.venv/bin/activate;
        uv sync --frozen;
        exec bash
      "
    stdin_open: true
    tty: true
    networks:
    - benchmark-net

  # Run unit tests in container (fast, no network)
  test:
    build:
      context: ..
      dockerfile: docker/Dockerfile.cpu
    image: benchmark-harness:test
    container_name: benchmark-harness-test
    volumes:
    - ../tests:/app/tests:ro
    - ../src:/app/src:ro
    - ../pyproject.toml:/app/pyproject.toml:ro
    - ../uv.lock:/app/uv.lock:ro
    - ../README.md:/app/README.md:ro
    - test-results:/app/results
    environment:
    - PYTHONDONTWRITEBYTECODE=1
    entrypoint: ["bash", "-c"]
    # Note: sqlite-vec 0.1.6 has broken ARM64 wheel; reinstall prerelease after uv sync
    # Run only unit tests (not integration tests which require git repos)
    command: ["uv sync --frozen --dev && uv pip install --force-reinstall --prerelease=allow sqlite-vec && pytest tests/unit/ -v --tb=short --cov=src --cov-report=xml:/app/results/coverage.xml"]
    networks:
    - benchmark-net

  # Run e2e tests in container (requires network for dataset downloads)
  test-e2e:
    build:
      context: ..
      dockerfile: docker/Dockerfile.cpu
    image: benchmark-harness:test-e2e
    container_name: benchmark-harness-test-e2e
    volumes:
    - ../tests:/app/tests:ro
    - ../src:/app/src:ro
    - ../pyproject.toml:/app/pyproject.toml:ro
    - ../uv.lock:/app/uv.lock:ro
    - ../README.md:/app/README.md:ro
    - ../data:/app/data:rw
    - test-results:/app/results
    environment:
    - PYTHONDONTWRITEBYTECODE=1
    - OPENAI_API_KEY=${OPENAI_API_KEY:-}
    entrypoint: ["bash", "-c"]
    # Note: sqlite-vec 0.1.6 has broken ARM64 wheel; reinstall prerelease after uv sync
    command: ["uv sync --frozen --dev && uv pip install --force-reinstall --prerelease=allow sqlite-vec && pytest tests/ -v --tb=short -m e2e"]
    networks:
    - benchmark-net

networks:
  benchmark-net:
    driver: bridge

volumes:
  # Persistent cache for HuggingFace models (avoids re-downloading)
  benchmark-cache:
    driver: local
  # Persistent venv for dev container
  dev-venv:
    driver: local
  # Test results volume
  test-results:
    driver: local
