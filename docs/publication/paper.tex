\documentclass[11pt]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{xcolor}

% Title
\title{Git-Notes Memory: A Git-Based Memory System for LLM Agents\\Evaluation Across Multiple Benchmarks}

\author{
  Author Name$^{1}$ \\
  $^{1}$Affiliation \\
  \texttt{email@example.com}
}

\date{}

\begin{document}

\maketitle

% =============================================================================
\begin{abstract}
% =============================================================================
We present a comprehensive evaluation of git-notes-memory-manager, a novel memory system for LLM agents that leverages Git's native note storage mechanism.
Our evaluation spans five established benchmarks: LongMemEval, LoCoMo, Context-Bench, MemoryAgentBench, and Terminal-Bench 2.0, covering \textit{N} question-answering tasks and \textit{M} agentic scenarios.
We compare git-notes memory against a no-memory baseline across multiple dimensions: accuracy, abstention rate, latency, and cost efficiency.
Results demonstrate that git-notes memory achieves \textbf{XX\%} higher accuracy on multi-hop reasoning tasks while maintaining sub-\textit{Y}ms retrieval latency.
Ablation studies reveal that semantic search and version history tracking contribute most significantly to performance gains.
Human validation with \textit{K} annotators confirms an inter-annotator agreement of $\kappa = $ \textit{Z}.
All code, data, and evaluation scripts are publicly available at \url{https://github.com/zircote/memory-benchmark-harness}.
\end{abstract}

% =============================================================================
\section{Introduction}
\label{sec:introduction}
% =============================================================================

Large Language Model (LLM) agents increasingly require persistent memory to maintain context across sessions, recall user preferences, and track evolving information.
While in-context learning provides short-term memory within a conversation, long-term memory systems face unique challenges: efficient storage and retrieval, handling conflicting information, and maintaining temporal consistency.

Existing approaches to LLM memory fall into several categories:
\begin{enumerate}
    \item \textbf{Vector databases} (e.g., Pinecone, Weaviate) store embeddings for semantic similarity search
    \item \textbf{Key-value stores} (e.g., Redis, MemoryGPT) provide fast lookup but limited semantic understanding
    \item \textbf{Structured knowledge graphs} (e.g., Neo4j-backed systems) capture relationships but require complex schemas
    \item \textbf{Document stores} (e.g., MongoDB) offer flexibility but lack temporal versioning
\end{enumerate}

We introduce git-notes-memory-manager, a memory system built on Git's native note storage mechanism.
Git notes provide several unique advantages for LLM memory:
\begin{itemize}
    \item \textbf{Version history}: Every memory update is tracked with full change history
    \item \textbf{Conflict resolution}: Git's merge strategies handle concurrent updates
    \item \textbf{Immutable audit trail}: Complete provenance for all stored information
    \item \textbf{Distributed by design}: Native support for synchronization across agents
\end{itemize}

Our contributions are:
\begin{enumerate}
    \item A comprehensive evaluation framework covering 5 benchmarks and 4 memory competencies
    \item Empirical comparison of git-notes memory against baseline conditions
    \item Ablation studies isolating the contribution of individual memory components
    \item Human validation protocol with inter-annotator agreement metrics
    \item Open-source benchmark harness for reproducible memory system evaluation
\end{enumerate}

% =============================================================================
\section{Related Work}
\label{sec:related}
% =============================================================================

\subsection{LLM Memory Systems}

MemGPT \cite{packer2023memgpt} introduces a hierarchical memory architecture with main memory and archival storage.
Letta (formerly MemGPT) extends this with tool-based memory operations.
Our evaluation includes Context-Bench from the Letta framework.

MemoryBank \cite{zhong2024memorybank} proposes a memory mechanism inspired by Ebbinghaus forgetting curves.
Similar to our approach, it emphasizes temporal dynamics of memory storage and retrieval.

Reflexion \cite{shinn2023reflexion} uses episodic memory to enable agents to learn from trial-and-error.
While focused on task-specific learning, the underlying memory requirements align with our evaluation criteria.

\subsection{Memory Evaluation Benchmarks}

\textbf{LongMemEval} \cite{wu2024longmemeval} provides a comprehensive benchmark for long-context memory in chat assistants.
It includes 500 questions across single-session and multi-session scenarios with 5 ability dimensions: knowledge extraction, temporal understanding, multi-hop reasoning, and more.

\textbf{LoCoMo} \cite{maharana2024locomo} offers a long conversation benchmark with 10 conversations (8.5k+ turns each) and 867 questions spanning 5 QA categories including adversarial queries.

\textbf{MemoryAgentBench} \cite{memoryagentbench2024} evaluates 4 core memory competencies: knowledge updates, temporal reasoning, belief maintenance, and conflict resolution.

\textbf{Terminal-Bench 2.0} \cite{terminalbench2024} assesses agentic capabilities in realistic development environments with multi-step tasks.

\subsection{Git for AI Systems}

Git has been explored for AI versioning in several contexts.
DVC (Data Version Control) extends Git for ML pipelines.
MLflow and Weights \& Biases track experiments but focus on model artifacts rather than runtime memory.
Our approach uniquely applies Git notes—a lesser-known Git feature—for agent memory storage.

% =============================================================================
\section{Git-Notes Memory System}
\label{sec:system}
% =============================================================================

\subsection{Architecture}

The git-notes-memory-manager consists of three primary components:

\begin{enumerate}
    \item \textbf{Storage Layer}: Git notes attached to a designated anchor commit, organized by namespace
    \item \textbf{Semantic Index}: Embedding-based index for similarity search across memories
    \item \textbf{Retrieval Engine}: Hybrid search combining semantic similarity and metadata filters
\end{enumerate}

\subsection{Memory Operations}

The system exposes four core operations through a standardized API:

\begin{itemize}
    \item \texttt{store(key, content, metadata)} -- Store a memory with optional metadata
    \item \texttt{retrieve(query, limit, filters)} -- Retrieve relevant memories
    \item \texttt{delete(key)} -- Remove a memory (creates deletion record)
    \item \texttt{history(key)} -- Return version history for a memory key
\end{itemize}

\subsection{Version History Tracking}

Unlike traditional key-value stores, git-notes memory maintains complete version history:

\begin{enumerate}
    \item Each \texttt{store()} operation creates a new commit in the notes ref
    \item Previous values are accessible via \texttt{history()}
    \item Conflicts between concurrent updates are resolved using Git's merge strategies
    \item Deletion creates a tombstone commit, preserving the audit trail
\end{enumerate}

% =============================================================================
\section{Evaluation Framework}
\label{sec:evaluation}
% =============================================================================

\subsection{Experimental Setup}

We evaluate git-notes memory across five benchmarks using a common harness:

\begin{table}[h]
\centering
\caption{Benchmark overview}
\label{tab:benchmarks}
\begin{tabular}{lcccc}
\toprule
\textbf{Benchmark} & \textbf{Tasks} & \textbf{Type} & \textbf{Focus} \\
\midrule
LongMemEval & 500 & QA & Long-context recall \\
LoCoMo & 867 & QA & Conversation memory \\
Context-Bench & TBD & QA & Context utilization \\
MemoryAgentBench & TBD & QA & Memory competencies \\
Terminal-Bench 2.0 & TBD & Agentic & Real-world tasks \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Adapter Conditions}

We compare three conditions:

\begin{enumerate}
    \item \textbf{git\_notes}: Full git-notes-memory-manager with semantic search
    \item \textbf{no\_memory}: Baseline without any external memory
    \item \textbf{Ablations}: Variants with individual components disabled
\end{enumerate}

\subsection{Ablation Study Design}

To isolate the contribution of each component, we evaluate five ablation conditions:

\begin{table}[h]
\centering
\caption{Ablation conditions}
\label{tab:ablations}
\begin{tabular}{ll}
\toprule
\textbf{Ablation} & \textbf{Description} \\
\midrule
no\_semantic\_search & Disable embedding-based retrieval \\
no\_metadata\_filter & Disable metadata filtering \\
no\_version\_history & Disable version history access \\
fixed\_window & Fixed 10-item retrieval window \\
recency\_only & Retrieve only most recent memories \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Metrics}

For each benchmark, we compute:

\begin{itemize}
    \item \textbf{Accuracy}: Correct responses / Total questions
    \item \textbf{Precision, Recall, F1}: Standard classification metrics
    \item \textbf{Abstention Rate}: Proportion of ``I don't know'' responses
    \item \textbf{Latency}: Memory operation time (ms)
    \item \textbf{Cost}: Estimated API cost per 1000 queries
\end{itemize}

Statistical significance is assessed using paired t-tests with Holm-Bonferroni correction for multiple comparisons.
Effect sizes are reported as Cohen's d.
95\% confidence intervals are computed using BCa bootstrap with 2000 iterations.

\subsection{LLM-as-Judge Protocol}

Response correctness is evaluated using GPT-4o as a judge with the following criteria:

\begin{enumerate}
    \item \textbf{Factual accuracy}: Does the response contain correct information?
    \item \textbf{Completeness}: Does it address all aspects of the question?
    \item \textbf{Relevance}: Is the response focused on the query?
    \item \textbf{Abstention appropriateness}: Is ``I don't know'' justified?
\end{enumerate}

Judgments are cached (content-addressed, SHA-256) with 30-day TTL to ensure reproducibility and reduce costs.

% =============================================================================
\section{Results}
\label{sec:results}
% =============================================================================

% NOTE: This section contains placeholder values. Replace with actual results.

\subsection{Main Results}

\begin{table}[h]
\centering
\caption{Main results across benchmarks. Best results in \textbf{bold}.}
\label{tab:main-results}
\begin{tabular}{lccccc}
\toprule
\textbf{Condition} & \textbf{LME} & \textbf{LoCoMo} & \textbf{CB} & \textbf{MAB} & \textbf{Overall} \\
\midrule
git\_notes & \textbf{XX.X\%} & \textbf{XX.X\%} & \textbf{XX.X\%} & \textbf{XX.X\%} & \textbf{XX.X\%} \\
no\_memory & XX.X\% & XX.X\% & XX.X\% & XX.X\% & XX.X\% \\
\midrule
$\Delta$ & +X.X\% & +X.X\% & +X.X\% & +X.X\% & +X.X\% \\
\bottomrule
\end{tabular}
\end{table}

% Placeholder for actual results narrative
\textit{[Results narrative to be added after experiments complete]}

\subsection{Ablation Study}

\begin{table}[h]
\centering
\caption{Ablation study results. $\Delta$ from full git\_notes baseline.}
\label{tab:ablation}
\begin{tabular}{lcccc}
\toprule
\textbf{Ablation} & \textbf{Accuracy} & \textbf{$\Delta$} & \textbf{$\Delta$\%} & \textbf{Sig.} \\
\midrule
Full system & XX.X\% & -- & -- & -- \\
w/o Semantic Search & XX.X\% & -X.X\% & -XX.X\% & $\checkmark$ \\
w/o Metadata Filter & XX.X\% & -X.X\% & -XX.X\% & $\checkmark$ \\
w/o Version History & XX.X\% & -X.X\% & -XX.X\% & \\
Fixed Window & XX.X\% & -X.X\% & -XX.X\% & $\checkmark$ \\
Recency Only & XX.X\% & -X.X\% & -XX.X\% & $\checkmark$ \\
\bottomrule
\end{tabular}
\end{table}

% Placeholder for ablation analysis
\textit{[Ablation analysis to be added after experiments complete]}

\subsection{Category Breakdown}

\begin{table}[h]
\centering
\caption{Performance by question category (LoCoMo).}
\label{tab:categories}
\begin{tabular}{lcc}
\toprule
\textbf{Category} & \textbf{git\_notes} & \textbf{no\_memory} \\
\midrule
Single-Hop QA & XX.X\% & XX.X\% \\
Multi-Hop QA & XX.X\% & XX.X\% \\
Temporal QA & XX.X\% & XX.X\% \\
Open-Domain QA & XX.X\% & XX.X\% \\
Adversarial QA & XX.X\% & XX.X\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Human Validation}

To validate LLM-as-Judge reliability, we conducted human annotation of N samples:

\begin{table}[h]
\centering
\caption{Human validation agreement metrics.}
\label{tab:human}
\begin{tabular}{lccc}
\toprule
\textbf{Benchmark} & \textbf{Agreement} & \textbf{$\kappa$} & \textbf{Weighted $\kappa$} \\
\midrule
LongMemEval & XX.X\% & 0.XXX & 0.XXX \\
LoCoMo & XX.X\% & 0.XXX & 0.XXX \\
Context-Bench & XX.X\% & 0.XXX & 0.XXX \\
\bottomrule
\end{tabular}
\end{table}

% =============================================================================
\section{Discussion}
\label{sec:discussion}
% =============================================================================

\subsection{Key Findings}

% Placeholder for discussion
\textit{[Discussion to be added after experiments complete]}

\subsection{Limitations}

Our evaluation has several limitations:

\begin{enumerate}
    \item \textbf{LLM-as-Judge bias}: GPT-4o judgments may favor certain response styles
    \item \textbf{Benchmark coverage}: Existing benchmarks may not capture all memory use cases
    \item \textbf{Single memory system}: Comparison against other memory systems (e.g., vector DBs) is future work
    \item \textbf{Cost considerations}: Git operations may have different scaling characteristics
\end{enumerate}

\subsection{Future Work}

Future directions include:

\begin{enumerate}
    \item Multi-agent memory sharing using Git's distributed architecture
    \item Comparative evaluation against vector database memory systems
    \item Integration with additional LLM providers beyond OpenAI
    \item Long-running agent scenarios over days/weeks
\end{enumerate}

% =============================================================================
\section{Conclusion}
\label{sec:conclusion}
% =============================================================================

We presented a comprehensive evaluation of git-notes-memory-manager across five established benchmarks.
Our results demonstrate that Git-based memory provides significant advantages for long-term agent memory, particularly for tasks requiring temporal reasoning and conflict resolution.
The complete benchmark harness is available at \url{https://github.com/zircote/memory-benchmark-harness} to facilitate reproducible evaluation of memory systems.

% =============================================================================
% Bibliography
% =============================================================================
\bibliographystyle{plain}
\begin{thebibliography}{10}

\bibitem{packer2023memgpt}
Charles Packer, Vivian Fang, Shishir G. Patil, Kevin Lin, Sarah Wooders, Joseph E. Gonzalez.
\newblock MemGPT: Towards LLMs as Operating Systems.
\newblock \textit{arXiv preprint arXiv:2310.08560}, 2023.

\bibitem{wu2024longmemeval}
Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, Yue Zhang.
\newblock LongMemEval: Benchmarking Long-Term Memory in Long-Context Language Models.
\newblock \textit{arXiv preprint}, 2024.

\bibitem{maharana2024locomo}
Aditi Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, Yuwei Fang.
\newblock LoCoMo: Long Context Multi-Turn Open-Domain Conversation Dataset.
\newblock \textit{arXiv preprint}, 2024.

\bibitem{zhong2024memorybank}
Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, Yanlin Wang.
\newblock MemoryBank: Enhancing Large Language Models with Long-Term Memory.
\newblock \textit{arXiv preprint arXiv:2305.10250}, 2024.

\bibitem{shinn2023reflexion}
Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao.
\newblock Reflexion: Language Agents with Verbal Reinforcement Learning.
\newblock \textit{NeurIPS}, 2023.

\bibitem{memoryagentbench2024}
MemoryAgentBench Authors.
\newblock MemoryAgentBench: Evaluating Memory Competencies in LLM Agents.
\newblock \textit{arXiv preprint}, 2024.

\bibitem{terminalbench2024}
Terminal-Bench Authors.
\newblock Terminal-Bench 2.0: Evaluating Agentic Capabilities.
\newblock \textit{arXiv preprint}, 2024.

\end{thebibliography}

\appendix

% =============================================================================
\section{Appendix: Benchmark Details}
\label{app:benchmarks}
% =============================================================================

\subsection{LongMemEval Question Types}

LongMemEval includes five question types:

\begin{enumerate}
    \item \textbf{Knowledge Extraction}: Direct recall of stated facts
    \item \textbf{Temporal Understanding}: Questions about time-related events
    \item \textbf{Multi-Hop Reasoning}: Questions requiring chaining information
    \item \textbf{Preference Tracking}: User preference and opinion recall
    \item \textbf{Belief Update}: Tracking information that changes over time
\end{enumerate}

\subsection{LoCoMo QA Categories}

LoCoMo defines five question categories:

\begin{enumerate}
    \item \textbf{Single-Hop QA}: Direct factual questions
    \item \textbf{Multi-Hop QA}: Questions requiring multiple memory lookups
    \item \textbf{Temporal QA}: Time-sensitive questions
    \item \textbf{Open-Domain QA}: General knowledge questions
    \item \textbf{Adversarial QA}: Questions designed to probe memory failures
\end{enumerate}

\subsection{MemoryAgentBench Competencies}

MemoryAgentBench evaluates four competencies:

\begin{enumerate}
    \item \textbf{Knowledge Updates}: Handling new or changed information
    \item \textbf{Temporal Reasoning}: Understanding time relationships
    \item \textbf{Belief Maintenance}: Consistency of stored beliefs
    \item \textbf{Conflict Resolution}: Handling contradictory information
\end{enumerate}

% =============================================================================
\section{Appendix: Annotation Guidelines}
\label{app:annotation}
% =============================================================================

\subsection{Rubric Levels}

Annotators used a 5-level rubric:

\begin{enumerate}
    \item \textbf{CORRECT} (1.0): Factually correct and complete
    \item \textbf{PARTIALLY\_CORRECT} (0.75): Mostly correct with minor issues
    \item \textbf{PARTIALLY\_INCORRECT} (0.25): Some correct elements but major issues
    \item \textbf{INCORRECT} (0.0): Factually incorrect
    \item \textbf{ABSTAINED} (N/A): Response was ``I don't know''
\end{enumerate}

\subsection{Decision Boundaries}

\begin{itemize}
    \item CORRECT vs PARTIALLY\_CORRECT: Are there any factual errors or omissions?
    \item PARTIALLY\_CORRECT vs PARTIALLY\_INCORRECT: Is the core claim correct?
    \item PARTIALLY\_INCORRECT vs INCORRECT: Is any substantive information correct?
\end{itemize}

% =============================================================================
\section{Appendix: Statistical Methods}
\label{app:statistics}
% =============================================================================

\subsection{Bootstrap Confidence Intervals}

We use BCa (bias-corrected and accelerated) bootstrap with 2000 iterations to compute 95\% confidence intervals.

\subsection{Multiple Comparison Correction}

For pairwise comparisons across benchmarks, we apply Holm-Bonferroni correction to control family-wise error rate at $\alpha = 0.05$.

\subsection{Effect Size Interpretation}

Cohen's d effect sizes are interpreted as:
\begin{itemize}
    \item Small: $|d| < 0.2$
    \item Medium: $0.2 \leq |d| < 0.8$
    \item Large: $|d| \geq 0.8$
\end{itemize}

\end{document}
